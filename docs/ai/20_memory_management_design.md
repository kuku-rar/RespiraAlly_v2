# AI Conversation Memory Management Design

---

**Document Version:** v1.0
**Last Updated:** 2025-10-18
**Author:** Claude Code AI - System Architect
**Status:** Draft
**Related Documents:**
- [Architecture & Design](../05_architecture_and_design.md) - System Architecture
- [AI Safety & Compliance](./19_ai_safety_and_compliance.md) - Security Principles
- [API Design Specification](../06_api_design_specification.md) - API Contracts
- [ADR-002: pgvector for Vector DB](../adr/ADR-002-pgvector-for-vector-db.md) - Long-term RAG storage
- [ADR-003: MongoDB for Event Logs](../adr/ADR-003-mongodb-for-event-logs.md) - Audit trail

---

## ç›®éŒ„ (Table of Contents)

- [A. Memory Architecture Overview](#a-memory-architecture-overview)
- [B. Short-term Memory (Redis)](#b-short-term-memory-redis)
- [C. Deduplication Mechanism](#c-deduplication-mechanism)
- [D. Rolling Summary Strategy](#d-rolling-summary-strategy)
- [E. Memory Gate Decision Logic](#e-memory-gate-decision-logic)
- [F. Audio-level Idempotency](#f-audio-level-idempotency)
- [G. Long-term Memory (PostgreSQL + pgvector)](#g-long-term-memory-postgresql--pgvector)
- [H. TTL & Data Retention Policy](#h-ttl--data-retention-policy)
- [I. Performance & Scalability](#i-performance--scalability)
- [J. Security & Privacy Considerations](#j-security--privacy-considerations)
- [K. Monitoring & Observability](#k-monitoring--observability)
- [L. Migration from V1](#l-migration-from-v1)
- [M. Review Conclusion & Action Items](#m-review-conclusion--action-items)

---

## A. Memory Architecture Overview

### A.1 è¨˜æ†¶é«”æ¶æ§‹å±¤æ¬¡ (Memory Architecture Layers)

RespiraAlly V2.0 æ¡ç”¨**ä¸‰å±¤è¨˜æ†¶é«”æ¶æ§‹** (Three-tier Memory Architecture) ä¾†å¹³è¡¡æ•ˆèƒ½ã€æˆæœ¬èˆ‡ä½¿ç”¨è€…é«”é©—:

```mermaid
graph LR
    subgraph "Layer 1: Short-term Memory (Redis)"
        A[Session State<br/>5min TTL]
        B[Conversation History<br/>24h TTL]
        C[Rolling Summary<br/>24h TTL]
        D[Audio Cache<br/>1h TTL]
    end

    subgraph "Layer 2: Mid-term Memory (PostgreSQL)"
        E[Conversation Logs<br/>30 days]
        F[User Preferences<br/>Permanent]
    end

    subgraph "Layer 3: Long-term Memory (pgvector)"
        G[Semantic Embeddings<br/>180 days]
        H[Important Moments<br/>Permanent]
    end

    A --> E
    B --> E
    C --> G
    E --> G

    style A fill:#ff9999
    style B fill:#ff9999
    style C fill:#ff9999
    style D fill:#ff9999
    style E fill:#99ccff
    style F fill:#99ccff
    style G fill:#99ff99
    style H fill:#99ff99
```

**Layer 1 (Redis)**: è¶…é«˜é€ŸçŸ­æœŸè¨˜æ†¶é«”ï¼Œæ”¯æ´ AI Worker å¯¦æ™‚æ±ºç­–
**Layer 2 (PostgreSQL)**: é—œè¯å¼çµæ§‹åŒ–å„²å­˜ï¼Œæ”¯æ´å¾Œè‡ºåˆ†æèˆ‡æ²»ç™‚å¸«æŸ¥è©¢
**Layer 3 (pgvector)**: å‘é‡èªç¾©æœå°‹ï¼Œæ”¯æ´ RAG çŸ¥è­˜æª¢ç´¢èˆ‡å€‹äººåŒ–å›æ†¶

### A.2 è¨­è¨ˆåŸå‰‡ (Design Principles)

åŸºæ–¼ [V1 beloved_grandson å°ˆæ¡ˆ](https://github.com/example/beloved_grandson) çš„æˆåŠŸç¶“é©—ï¼ŒV2.0 ç¹¼æ‰¿ä»¥ä¸‹æ ¸å¿ƒè¨­è¨ˆåŸå‰‡:

1. **Ephemeral First (çŸ­æš«å„ªå…ˆ)**: é è¨­æ‰€æœ‰è¨˜æ†¶é«”è³‡æ–™ç‚ºçŸ­æš«çš„ (TTL < 24h)ï¼Œé™¤éæ˜ç¢ºæ¨™è¨˜ç‚ºæ°¸ä¹…
2. **Deduplication by Design (è¨­è¨ˆå³å»é‡)**: ä½¿ç”¨æ™‚é–“çª—å£ + å…§å®¹é›œæ¹Šé˜²æ­¢é‡è¤‡è™•ç†
3. **Gradual Compression (æ¼¸é€²å¼å£“ç¸®)**: ä½¿ç”¨æ»¾å‹•æ‘˜è¦ (rolling summary) é¿å…è¨˜æ†¶é«”çˆ†ç‚¸
4. **Privacy by Default (é è¨­éš±ç§)**: æ•æ„Ÿè³‡æ–™åŠ å¯†å„²å­˜ï¼Œæ”¯æ´ä½¿ç”¨è€…ä¸»å‹•æ¸…é™¤
5. **Idempotent Operations (å†ªç­‰æ“ä½œ)**: éŸ³æª”ç´šé–ä¿è­‰åŒä¸€æ®µèªéŸ³åªè™•ç†ä¸€æ¬¡

---

## B. Short-term Memory (Redis)

### B.1 Redis Key è¨­è¨ˆè¦ç¯„ (Key Design Schema)

éµå¾ª **å‘½åç©ºé–“ + å¯¦é«” + å±¬æ€§** çš„ä¸‰æ®µå¼è¨­è¨ˆï¼Œç¢ºä¿å¯è®€æ€§èˆ‡å¯ç¶­è­·æ€§:

| Key Pattern | ç”¨é€” | TTL | è³‡æ–™çµæ§‹ | ç¯„ä¾‹ |
|-------------|------|-----|----------|------|
| `session:{user_id}:state` | Session ç‹€æ…‹ (ACTIVE/IDLE) | 5min | String | `session:U123:state` â†’ "ACTIVE" |
| `session:{user_id}:history` | å°è©±æ­·å² (åŸå§‹ JSON) | 24h | List | `session:U123:history` â†’ [{input, output, rid}...] |
| `session:{user_id}:summary:text` | æ»¾å‹•æ‘˜è¦æ–‡å­— | 24h | String | `session:U123:summary:text` â†’ "ä½¿ç”¨è€…è¿‘æœŸç„¦æ…®..." |
| `session:{user_id}:summary:rounds` | å·²æ‘˜è¦çš„è¼ªæ•¸ | 24h | Integer | `session:U123:summary:rounds` â†’ 10 |
| `processed:{user_id}:{request_id}` | å»é‡æ¨™è¨˜ | 24h | String | `processed:U123:abc123` â†’ "1" |
| `audio:{user_id}:{audio_id}:buf` | èªéŸ³ç‰‡æ®µç·©è¡å€ | 1h | List | `audio:U123:aud456:buf` â†’ ["ä½ å¥½", "è«‹å•"] |
| `audio:{user_id}:{audio_id}:result` | èªéŸ³è™•ç†çµæœå¿«å– | 24h | String | `audio:U123:aud456:result` â†’ "é˜¿å…¬ä½ å¥½..." |
| `lock:audio:{lock_id}` | éŸ³æª”ç´šé– (å†ªç­‰æ€§ä¿è­‰) | 3min | String | `lock:audio:U123#audio:aud456` â†’ "1" |

**Key è¨­è¨ˆæœ€ä½³å¯¦è¸**:
- âœ… ä½¿ç”¨ `:` ä½œç‚ºå±¤ç´šåˆ†éš”ç¬¦ (ç¬¦åˆ Redis æ…£ä¾‹)
- âœ… ç¬¬ä¸€æ®µç‚ºå‘½åç©ºé–“ (session, audio, lock, processed)
- âœ… ç¬¬äºŒæ®µç‚ºä½¿ç”¨è€…è­˜åˆ¥ (æ”¯æ´æŒ‰ä½¿ç”¨è€…å¿«é€Ÿæ¸…é™¤)
- âœ… é¿å…ä½¿ç”¨ `.` æˆ– `/` (æ˜“èˆ‡æª”æ¡ˆç³»çµ±æ··æ·†)
- âŒ ç¦æ­¢åœ¨ Key ä¸­å„²å­˜æ•æ„Ÿè³‡æ–™ (PII)

### B.2 Session State Management (æœƒè©±ç‹€æ…‹ç®¡ç†)

**ç‹€æ…‹è½‰æ›åœ–** (State Transition Diagram):

```mermaid
stateDiagram-v2
    [*] --> ACTIVE: User sends message
    ACTIVE --> ACTIVE: Activity (Touch TTL)
    ACTIVE --> [*]: 5min idle (TTL expires)
    ACTIVE --> IDLE: Manual pause (Future)
    IDLE --> ACTIVE: Resume conversation
```

**å¯¦ä½œç´°ç¯€**:

```python
# V2.0 åƒè€ƒ V1 å¯¦ä½œ
def ensure_active_state(user_id: str) -> None:
    """ç¢ºä¿ Session è™•æ–¼ ACTIVE ç‹€æ…‹"""
    r = get_redis()
    key = f"session:{user_id}:state"
    r.set(key, "ACTIVE", nx=True)  # nx=True: åªåœ¨ä¸å­˜åœ¨æ™‚è¨­å®š
    _touch_ttl([key])  # å»¶é•· TTL åˆ° 5min

def _touch_ttl(keys: List[str]) -> None:
    """æ‰¹æ¬¡å»¶é•· TTL (é¿å…å¤šæ¬¡ RTT)"""
    if not keys:
        return
    r = get_redis()
    p = r.pipeline()
    for k in keys:
        p.pexpire(k, SESSION_TTL_MS)  # 5min = 300000ms
    p.execute()
```

**é—œéµè¨­è¨ˆæ±ºç­–**:
- **NX èªç¾©**: ä½¿ç”¨ `SET key value NX` é¿å…è¦†è“‹æ—¢æœ‰ Session
- **æ‰¹æ¬¡ TTL æ›´æ–°**: ä½¿ç”¨ Redis Pipeline æ¸›å°‘ç¶²è·¯å¾€è¿” (RTT)
- **5 åˆ†é˜é–’ç½®è¶…æ™‚**: å¹³è¡¡ä½¿ç”¨è€…é«”é©—èˆ‡è¨˜æ†¶é«”æˆæœ¬ (å¯æ ¹æ“šç›£æ§æ•¸æ“šèª¿æ•´)

### B.3 Conversation History Storage (å°è©±æ­·å²å„²å­˜)

**è³‡æ–™çµæ§‹** (æ¡ç”¨ Redis List):

```python
# Key: session:{user_id}:history
# Value: List[JSON] - ä¿æŒæ’å…¥é †åºï¼Œæ”¯æ´é«˜æ•ˆ LPUSH/RPUSH/LRANGE

# å–®ç­†å°è©±ç‰©ä»¶ Schema
{
    "input": str,         # ä½¿ç”¨è€…è¼¸å…¥ (åŸæ–‡)
    "output": str,        # AI å›æ‡‰ (åŸæ–‡)
    "rid": str,           # Request ID (å»é‡ç”¨)
    "timestamp": int,     # Unix timestamp (ms)
    "is_emergency": bool  # æ˜¯å¦è§¸ç™¼ç·Šæ€¥é€šå ± (é¸å¡«)
}
```

**å¯¦ä½œç¯„ä¾‹**:

```python
def append_round(user_id: str, round_obj: Dict) -> None:
    """è¿½åŠ ä¸€è¼ªå°è©±åˆ°æ­·å²è¨˜éŒ„"""
    r = get_redis()
    key = f"session:{user_id}:history"
    r.rpush(key, json.dumps(round_obj, ensure_ascii=False))

    # åŒæ™‚å»¶é•·æ‰€æœ‰ç›¸é—œ Key çš„ TTL
    ensure_active_state(user_id)
    _touch_ttl([
        key,
        f"session:{user_id}:summary:text",
        f"session:{user_id}:summary:rounds",
        f"session:{user_id}:state",
    ])

def fetch_unsummarized_tail(user_id: str, k: int = 6) -> List[Dict]:
    """æ“·å–æœ€è¿‘ k è¼ªæœªæ‘˜è¦çš„å°è©± (ç”¨æ–¼ Prompt ä¸Šä¸‹æ–‡)"""
    r = get_redis()
    cursor = int(r.get(f"session:{user_id}:summary:rounds") or 0)
    items = r.lrange(f"session:{user_id}:history", cursor, -1)
    return [json.loads(x) for x in items[-k:]]
```

**ç‚ºä»€éº¼é¸æ“‡ List è€Œé Sorted Set?**

| å°æ¯”é … | List | Sorted Set |
|--------|------|------------|
| æ’å…¥é †åºä¿è­‰ | âœ… åŸç”Ÿæ”¯æ´ | âŒ éœ€é¡å¤– score æ¬„ä½ |
| ç¯„åœæŸ¥è©¢ | âœ… LRANGE O(N) | âœ… ZRANGE O(log N + M) |
| è¨˜æ†¶é«”é–‹éŠ· | âœ… è¼ƒå° | âŒ è¼ƒå¤§ (é¡å¤– score ç´¢å¼•) |
| ä½¿ç”¨å ´æ™¯ | âœ… è¿‘æœŸ N è¼ªå°è©± | âŒ ä¾æ™‚é–“æˆ³æŸ¥è©¢ |

**çµè«–**: å°è©±æ­·å²ä»¥**æ™‚é–“é †åº**ç‚ºä¸»è¦å­˜å–æ¨¡å¼ï¼ŒList æ›´ç°¡å–®é«˜æ•ˆã€‚

---

## C. Deduplication Mechanism

### C.1 æ™‚é–“çª—å£å»é‡ (Time-bucket Deduplication)

**æ ¸å¿ƒæ¦‚å¿µ**: å°‡æ™‚é–“è»¸åˆ‡åˆ†ç‚º **3 ç§’çª—å£**ï¼ŒåŒä¸€çª—å£å…§ç›¸åŒæ–‡å­—è¦–ç‚ºé‡è¤‡è«‹æ±‚ã€‚

**V1 å¯¦ä½œåƒè€ƒ** (ç¹¼æ‰¿è‡³ V2.0):

```python
def make_request_id(user_id: str, text: str, now_ms: Optional[int] = None) -> str:
    """ç”ŸæˆåŸºæ–¼æ™‚é–“çª—å£çš„ Request ID (SHA-1 é›œæ¹Š)"""
    if now_ms is None:
        now_ms = int(time.time() * 1000)

    # é—œéµ: å°‡æ¯«ç§’æ™‚é–“æˆ³é™¤ä»¥ 3000 å¾—åˆ°çª—å£ ID
    bucket = now_ms // 3000  # 3ç§’ = 3000ms

    # çµ„åˆ user_id + text + bucket è¨ˆç®—é›œæ¹Š
    return hashlib.sha1(f"{user_id}|{text}|{bucket}".encode()).hexdigest()

def try_register_request(user_id: str, request_id: str) -> bool:
    """å˜—è©¦è¨»å†Šè«‹æ±‚ (CAS èªç¾©)ï¼ŒæˆåŠŸå›å‚³ Trueï¼Œé‡è¤‡å›å‚³ False"""
    r = get_redis()
    key = f"processed:{user_id}:{request_id}"
    # SET NX: åªåœ¨ä¸å­˜åœ¨æ™‚è¨­å®šï¼ŒåŸå­æ€§ä¿è­‰
    return bool(r.set(key, "1", nx=True, ex=REDIS_TTL_SECONDS))
```

**å»é‡æµç¨‹åœ–**:

```mermaid
sequenceDiagram
    participant User
    participant API
    participant Redis
    participant AIWorker

    User->>API: POST /api/voice (audio_id=abc, text="é ­ç–¼")
    API->>API: make_request_id(U123, "é ­ç–¼", now=1000ms)
    Note right of API: bucket = 1000 // 3000 = 0<br/>rid = sha1("U123|é ­ç–¼|0")

    API->>Redis: SET NX processed:U123:rid "1"
    Redis-->>API: True (é¦–æ¬¡è«‹æ±‚)
    API->>AIWorker: è™•ç†ä»»å‹™

    User->>API: POST /api/voice (é‡è¤‡, text="é ­ç–¼")
    API->>API: make_request_id(U123, "é ­ç–¼", now=2500ms)
    Note right of API: bucket = 2500 // 3000 = 0 (åŒä¸€çª—å£!)

    API->>Redis: SET NX processed:U123:rid "1"
    Redis-->>API: False (Key å·²å­˜åœ¨)
    API-->>User: 200 OK (ç›´æ¥å›å‚³å¿«å–çµæœ)
```

**ç‚ºä»€éº¼é¸æ“‡ 3 ç§’?**

| çª—å£å¤§å° | å„ªé» | ç¼ºé» | é©ç”¨å ´æ™¯ |
|----------|------|------|----------|
| 1 ç§’ | å»é‡ç²¾åº¦é«˜ | æ­£å¸¸é‡è©¦å¯èƒ½è¢«èª¤åˆ¤ | é«˜é »äº’å‹• (èŠå¤©æ©Ÿå™¨äºº) |
| **3 ç§’** | **å¹³è¡¡ç²¾åº¦èˆ‡å®¹éŒ¯** | **- (æœ€ä½³é¸æ“‡)** | **COPD èªéŸ³äº’å‹•** |
| 5 ç§’ | å®¹éŒ¯é«˜ | çœŸå¯¦é‡è¤‡å¯èƒ½æ¼é | ä½é »äº’å‹• (è¡¨å–®æäº¤) |

**çµè«–**: 3 ç§’çª—å£ç¬¦åˆä»¥ä¸‹éœ€æ±‚:
1. âœ… LINE Webhook å¯èƒ½å› ç¶²è·¯æŠ–å‹•é‡é€ (é€šå¸¸ < 2s)
2. âœ… ä½¿ç”¨è€…ä¸å¤ªå¯èƒ½åœ¨ 3 ç§’å…§èªªå…©æ¬¡å®Œå…¨ç›¸åŒçš„è©±
3. âœ… å³ä½¿èª¤åˆ¤ï¼Œä½¿ç”¨è€…ä¹Ÿå¯é‡æ–°éŒ„éŸ³ (UX å¯æ¥å—)

### C.2 å»é‡èˆ‡å†ªç­‰æ€§çš„é—œä¿‚ (Deduplication vs Idempotency)

**å…©è€…å€åˆ¥**:

| æ¦‚å¿µ | ç›®çš„ | å¯¦ä½œå±¤ | ç¯„ä¾‹ |
|------|------|--------|------|
| **å»é‡ (Deduplication)** | é¿å…é‡è¤‡è™•ç†ç›¸åŒè«‹æ±‚ (æ•ˆèƒ½å„ªåŒ–) | Application Layer | 3s æ™‚é–“çª—å£ + Request ID |
| **å†ªç­‰æ€§ (Idempotency)** | ä¿è­‰å¤šæ¬¡åŸ·è¡Œçµæœä¸€è‡´ (æ­£ç¢ºæ€§ä¿è­‰) | Infrastructure Layer | éŸ³æª”ç´šé– + CAS |

**V2.0 æ¡ç”¨é›™å±¤é˜²è­·**:
1. **ç¬¬ä¸€å±¤**: Request ID å»é‡ â†’ å¿«é€Ÿéæ¿¾ (< 1ms)
2. **ç¬¬äºŒå±¤**: Audio Lock å†ªç­‰ â†’ åš´æ ¼ä¿è­‰ (è©³è¦‹ [Section F](#f-audio-level-idempotency))

---

## D. Rolling Summary Strategy

### D.1 ç‚ºä»€éº¼éœ€è¦æ»¾å‹•æ‘˜è¦? (Why Rolling Summary?)

**å•é¡Œ**: éš¨è‘—å°è©±è¼ªæ•¸å¢åŠ ï¼ŒPrompt é•·åº¦æœƒç·šæ€§å¢é•·ï¼Œå°è‡´:
- ğŸ’° **æˆæœ¬çˆ†ç‚¸**: GPT-4 Token è²»ç”¨èˆ‡é•·åº¦æˆæ­£æ¯”
- â±ï¸ **å»¶é²å¢åŠ **: æ›´é•·çš„ Prompt éœ€è¦æ›´å¤šæ¨ç†æ™‚é–“
- ğŸ“‰ **æ•ˆæœä¸‹é™**: LLM å°è¶…é•·ä¸Šä¸‹æ–‡çš„ä¸­é–“éƒ¨åˆ†æ³¨æ„åŠ›ä¸‹é™ (Lost in the Middle ç¾è±¡)

**è§£æ±ºæ–¹æ¡ˆ**: æ¯éš” **5 è¼ªå°è©±** è‡ªå‹•å°‡èˆŠå°è©±å£“ç¸®ç‚ºæ‘˜è¦ï¼Œåªä¿ç•™æœ€è¿‘ 6 è¼ªåŸæ–‡ + æ­·å²æ‘˜è¦ã€‚

### D.2 æ»¾å‹•æ‘˜è¦æ©Ÿåˆ¶ (Rolling Summary Mechanism)

**æ ¸å¿ƒè¨­è¨ˆ** (ç¹¼æ‰¿è‡ª V1):

```python
SUMMARY_CHUNK_SIZE = 5  # æ¯ 5 è¼ªå£“ç¸®ä¸€æ¬¡

def log_session(user_id: str, query: str, reply: str, request_id: str):
    """è¨˜éŒ„å°è©±ä¸¦è§¸ç™¼æ‘˜è¦ (å¦‚æœç´¯ç©è¶³å¤ è¼ªæ•¸)"""
    # 1. è¿½åŠ åˆ°æ­·å²è¨˜éŒ„
    append_round(user_id, {
        "input": query,
        "output": reply,
        "rid": request_id,
        "timestamp": int(time.time() * 1000)
    })

    # 2. æª¢æŸ¥æ˜¯å¦ç´¯ç©äº† 5 è¼ªæœªæ‘˜è¦çš„å°è©±
    start, chunk = peek_next_n(user_id, SUMMARY_CHUNK_SIZE)

    # 3. å¦‚æœæœ‰è¶³å¤ è¼ªæ•¸ï¼Œè§¸ç™¼ LLM æ‘˜è¦ä¸¦ CAS æäº¤
    if start is not None and chunk:
        summarize_chunk_and_commit(user_id, start_round=start, history_chunk=chunk)

def peek_next_n(user_id: str, n: int) -> Tuple[Optional[int], List[Dict]]:
    """çªºè¦–æ¥ä¸‹ä¾† n è¼ªæœªæ‘˜è¦çš„å°è©± (ä¸è¶³å‰‡å›å‚³ None)"""
    r = get_redis()
    cursor = int(r.get(f"session:{user_id}:summary:rounds") or 0)
    total = r.llen(f"session:{user_id}:history")

    if (total - cursor) < n:
        return None, []  # ä¸è¶³ n è¼ªï¼Œä¸è§¸ç™¼æ‘˜è¦

    items = r.lrange(f"session:{user_id}:history", cursor, cursor + n - 1)
    return cursor, [json.loads(x) for x in items]
```

**æ‘˜è¦æäº¤ä½¿ç”¨ CAS (Compare-And-Swap) ä¿è­‰åŸå­æ€§**:

```python
def commit_summary_chunk(
    user_id: str,
    expected_cursor: int,
    advance: int,
    add_text: str
) -> bool:
    """ä½¿ç”¨æ¨‚è§€é–æäº¤æ‘˜è¦ (é¿å…ä¸¦ç™¼ç«¶çˆ­)"""
    r = get_redis()
    ckey = f"session:{user_id}:summary:rounds"
    tkey = f"session:{user_id}:summary:text"

    with r.pipeline() as p:
        while True:
            try:
                # 1. Watch å…©å€‹ Key (æ¨‚è§€é–)
                p.watch(ckey, tkey)

                # 2. æª¢æŸ¥ cursor æ˜¯å¦ä»ç‚ºé æœŸå€¼
                cur = int(p.get(ckey) or 0)
                if cur != expected_cursor:
                    p.unwatch()
                    return False  # å…¶ä»–ç¨‹åºå·²æäº¤ï¼Œæ”¾æ£„æœ¬æ¬¡

                # 3. è®€å–èˆŠæ‘˜è¦ä¸¦é™„åŠ æ–°æ‘˜è¦
                old = p.get(tkey) or ""
                new = (old + "\n\n" + add_text.strip()) if add_text else old

                # 4. åŸå­æ€§æ›´æ–° (MULTI/EXEC)
                p.multi()
                p.set(tkey, new)
                p.set(ckey, cur + advance)
                p.execute()

                _touch_ttl([ckey, tkey])
                return True

            except redis.WatchError:
                return False  # å…¶ä»–äº‹å‹™ä¿®æ”¹äº† Keyï¼Œé‡è©¦
```

### D.3 æ‘˜è¦æ™‚æ©Ÿèˆ‡é »ç‡ (Summary Timing & Frequency)

**æ™‚æ©Ÿæ±ºç­–è¡¨**:

| å°è©±è¼ªæ•¸ | åŸå§‹è¨˜æ†¶ (Tokens) | æ‘˜è¦å¾Œ (Tokens) | ç¯€çœæ¯”ä¾‹ | æ˜¯å¦è§¸ç™¼æ‘˜è¦ |
|----------|-------------------|-----------------|----------|--------------|
| 0-4 è¼ª | ~200-800 | - | - | âŒ ä¸è§¸ç™¼ |
| 5 è¼ª | ~1000 | ~150 | **85%** | âœ… é¦–æ¬¡æ‘˜è¦ |
| 10 è¼ª | ~2000 | ~300 | **85%** | âœ… ç¬¬äºŒæ¬¡æ‘˜è¦ |
| 15 è¼ª | ~3000 | ~450 | **85%** | âœ… ç¬¬ä¸‰æ¬¡æ‘˜è¦ |

**é—œéµè§€å¯Ÿ**:
- ğŸ“Š **å£“ç¸®æ¯”ç©©å®š**: æ¯ 5 è¼ªå£“ç¸®å¯ç¯€çœ ~85% Tokens
- ğŸ’¡ **å†·å•Ÿå‹•å„ªåŒ–**: å‰ 4 è¼ªä¸è§¸ç™¼æ‘˜è¦ (é¿å…éæ—©æå¤±ç´°ç¯€)
- ğŸ”„ **éè¿´æ‘˜è¦**: æ‘˜è¦æœ¬èº«ä¹Ÿå¯è¢«å¾ŒçºŒæ‘˜è¦å£“ç¸® (é¡ä¼¼ Git commit squash)

**ç‚ºä»€éº¼é¸æ“‡ 5 è¼ªè€Œé 10 è¼ª?**

| Chunk Size | å„ªé» | ç¼ºé» | æ±ºç­– |
|------------|------|------|------|
| 3 è¼ª | Token ç¯€çœå¿« | ç´°ç¯€æå¤±å¤š | âŒ éæ–¼æ¿€é€² |
| **5 è¼ª** | **å¹³è¡¡ç´°ç¯€èˆ‡æˆæœ¬** | **- (æœ€ä½³)** | âœ… **æ¡ç”¨** |
| 10 è¼ª | ç´°ç¯€ä¿ç•™å¤š | Token æˆæœ¬é«˜ | âŒ æˆæœ¬éé«˜ |

### D.4 Prompt çµ„è£ç­–ç•¥ (Prompt Assembly)

**æœ€çµ‚ Prompt çµæ§‹** (é¤µçµ¦ LLM):

```python
def build_prompt_from_redis(
    user_id: str,
    k: int = 6,
    current_input: str = ""
) -> str:
    """çµ„è£ä¸Šä¸‹æ–‡ Prompt: æ­·å²æ‘˜è¦ + æœ€è¿‘ k è¼ª + ç•¶å‰è¼¸å…¥"""
    # 1. å–å¾—æ­·å²æ‘˜è¦ (å¯èƒ½ç‚ºç©º)
    summary_text, _ = get_summary(user_id)

    # 2. å–å¾—æœ€è¿‘ k è¼ªæœªæ‘˜è¦çš„å°è©±
    recent_rounds = fetch_unsummarized_tail(user_id, k=k)

    # 3. çµ„è£ Prompt
    prompt_parts = []

    if summary_text:
        prompt_parts.append(f"â­ æ­·å²å°è©±æ‘˜è¦:\n{summary_text}")

    if recent_rounds:
        recent_text = "\n".join([
            f"User: {r['input']}\nAssistant: {r['output']}"
            for r in recent_rounds
        ])
        prompt_parts.append(f"ğŸ“ æœ€è¿‘å°è©±:\n{recent_text}")

    if current_input:
        prompt_parts.append(f"ğŸ¤ ç•¶å‰è¼¸å…¥:\n{current_input}")

    return "\n\n".join(prompt_parts)
```

**Prompt ç¯„ä¾‹** (ç¬¬ 12 è¼ªå°è©±):

```
â­ æ­·å²å°è©±æ‘˜è¦:
ä½¿ç”¨è€…è¿‘æœŸåæ˜ å‘¼å¸å›°é›£åŠ åŠ‡ï¼Œå¤œé–“å’³å—½é »ç¹ã€‚å·²æé†’è¦å¾‹ç”¨è—¥èˆ‡ç›£æ¸¬ SpO2ã€‚
æƒ…ç·’ç„¦æ…®ï¼Œæ“”å¿ƒç—…æƒ…æƒ¡åŒ–ã€‚

ğŸ“ æœ€è¿‘å°è©±:
User: ä»Šå¤©èµ°è·¯æœƒå–˜
Assistant: é˜¿å…¬ï¼Œè‹¥æ˜¯è¡Œå‹•æ™‚æœƒå–˜ï¼Œè¨˜å¾—æ…¢æ…¢è¡Œï¼Œä¸é€šå‹‰å¼·ã€‚æœ‰é‡è¡€æ°§å—?
User: è¡€æ°§ 92
Assistant: 92% æœ‰é»åä½ï¼Œå»ºè­°ä½ ç”¨æ°§æ°£æ©Ÿï¼Œè‹¥æŒçºŒä½æ–¼ 90 è¦ç›¡å¿«æ›æ€¥è¨ºå–”ã€‚
...

ğŸ¤ ç•¶å‰è¼¸å…¥:
ç¾åœ¨èƒ¸å£æœ‰é»æ‚¶
```

**Token é ç®—æ§åˆ¶**:

| çµ„æˆéƒ¨åˆ† | Token ä¼°è¨ˆ | ä½”æ¯” | å‚™è¨» |
|----------|-----------|------|------|
| ç³»çµ±æç¤ºè© | ~300 | 15% | å›ºå®š |
| æ­·å²æ‘˜è¦ | ~200 | 10% | éš¨æ™‚é–“ç·©æ…¢å¢é•· |
| æœ€è¿‘ 6 è¼ª | ~1200 | 60% | ä¸»è¦ä¸Šä¸‹æ–‡ |
| ç•¶å‰è¼¸å…¥ | ~100 | 5% | è®Šå‹• |
| é ç•™ç·©è¡ | ~200 | 10% | å·¥å…·å‘¼å«ã€RAG æª¢ç´¢ |
| **ç¸½è¨ˆ** | **~2000** | **100%** | **é ä½æ–¼ GPT-4 Turbo 128k ä¸Šé™** |

---

## E. Memory Gate Decision Logic

### E.1 ä»€éº¼æ˜¯ Memory Gate? (What is Memory Gate?)

**æ¦‚å¿µ**: åœ¨æ¯æ¬¡å°è©±å‰ï¼Œä½¿ç”¨ **è¼•é‡ç´š LLM åˆ¤æ–·å™¨** æ±ºå®šæ˜¯å¦éœ€è¦æª¢ç´¢é•·æœŸè¨˜æ†¶ã€‚

**ç‚ºä»€éº¼éœ€è¦?**
- ğŸ’° **æˆæœ¬å„ªåŒ–**: æ¯æ¬¡éƒ½æª¢ç´¢è¨˜æ†¶æœƒå¢åŠ  Prompt é•·åº¦ â†’ æµªè²» Token
- â±ï¸ **å»¶é²å„ªåŒ–**: æª¢ç´¢ Redis + pgvector å¢åŠ  ~50-100ms å»¶é²
- ğŸ¯ **ç²¾æº–åº¦æå‡**: ä¸ç›¸é—œçš„è¨˜æ†¶åè€Œæœƒå¹²æ“¾ LLM åˆ¤æ–·

**V1 å¯¦ä½œåƒè€ƒ** (ç°¡åŒ–ç‰ˆ):

```python
class MemoryGateTool(BaseTool):
    name = "memory_gate"
    description = "åˆ¤æ–·ç•¶å‰å•é¡Œæ˜¯å¦éœ€è¦æª¢ç´¢æ­·å²å°è©±è¨˜æ†¶"

    def _run(self, query: str) -> str:
        """å›å‚³ 'USE' æˆ– 'SKIP'"""
        # å¿«é€Ÿè¦å‰‡ (è¦å‰‡å¼•æ“)
        if self._is_greeting(query):
            return "SKIP"  # æ‰“æ‹›å‘¼ä¸éœ€è¨˜æ†¶

        if self._contains_time_reference(query):
            return "USE"  # "ä¸Šæ¬¡ä½ èªª..." â†’ éœ€è¦è¨˜æ†¶

        if self._is_symptom_update(query):
            return "USE"  # ç—‡ç‹€è®ŠåŒ– â†’ éœ€è¦å°æ¯”æ­·å²

        # æ¨¡ç³Šæƒ…æ³ â†’ å‘¼å«è¼•é‡ LLM (gpt-4o-mini)
        return self._llm_decide(query)

    def _llm_decide(self, query: str) -> str:
        """ä½¿ç”¨ GPT-4o-mini å¿«é€Ÿåˆ¤æ–· (< 50ms)"""
        prompt = f"""åˆ¤æ–·ä»¥ä¸‹å•é¡Œæ˜¯å¦éœ€è¦åƒè€ƒæ­·å²å°è©±è¨˜æ†¶?

        å•é¡Œ: {query}

        éœ€è¦è¨˜æ†¶çš„æƒ…æ³:
        - è©¢å•ã€Œä¹‹å‰/ä¸Šæ¬¡/æœ€è¿‘ã€çš„äº‹æƒ…
        - ç—‡ç‹€è®ŠåŒ–å°æ¯” (ä»Šå¤© vs æ˜¨å¤©)
        - å€‹äººåŒ–å»ºè­° (éœ€è¦çŸ¥é“ä½¿ç”¨è€…åå¥½)

        ä¸éœ€è¦è¨˜æ†¶çš„æƒ…æ³:
        - æ‰“æ‹›å‘¼ã€é–’èŠ
        - ä¸€èˆ¬æ€§è¡›æ•™å•é¡Œ (å¦‚ "COPD æ˜¯ä»€éº¼?")
        - ç•¶ä¸‹ç—‡ç‹€æè¿° (ç„¡éœ€å°æ¯”)

        åªå›ç­” 'USE' æˆ– 'SKIP'ï¼Œä¸è¦è§£é‡‹ã€‚"""

        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=5
        )
        return response.choices[0].message.content.strip()
```

### E.2 æ±ºç­–æµç¨‹åœ– (Decision Flow)

```mermaid
flowchart TD
    Start[ä½¿ç”¨è€…è¼¸å…¥] --> Gate{Memory Gate åˆ¤æ–·}

    Gate -->|å¿«é€Ÿè¦å‰‡: æ‰“æ‹›å‘¼| SkipMemory[SKIP - ä¸æª¢ç´¢è¨˜æ†¶]
    Gate -->|å¿«é€Ÿè¦å‰‡: æ™‚é–“åƒè€ƒè©| UseMemory[USE - æª¢ç´¢è¨˜æ†¶]
    Gate -->|å¿«é€Ÿè¦å‰‡: ç—‡ç‹€æ›´æ–°| UseMemory
    Gate -->|æ¨¡ç³Šæƒ…æ³| LLM[å‘¼å« GPT-4o-mini]

    LLM --> LLMDecision{LLM åˆ¤æ–·}
    LLMDecision -->|USE| UseMemory
    LLMDecision -->|SKIP| SkipMemory

    SkipMemory --> BuildPrompt1[çµ„è£ Prompt<br/>åƒ…æ­·å²æ‘˜è¦ + ç•¶å‰è¼¸å…¥]
    UseMemory --> RetrieveRedis[æª¢ç´¢ Redis æœ€è¿‘ 6 è¼ª]
    RetrieveRedis --> RetrievePgvector{éœ€è¦ RAG?}
    RetrievePgvector -->|æ˜¯| RAG[æª¢ç´¢ pgvector èªç¾©ç›¸ä¼¼è¨˜æ†¶]
    RetrievePgvector -->|å¦| BuildPrompt2[çµ„è£ Prompt<br/>æ‘˜è¦ + 6è¼ª + ç•¶å‰]
    RAG --> BuildPrompt3[çµ„è£ Prompt<br/>æ‘˜è¦ + 6è¼ª + RAG + ç•¶å‰]

    BuildPrompt1 --> MainLLM[å‘¼å«ä¸» LLM<br/>ç”Ÿæˆå›æ‡‰]
    BuildPrompt2 --> MainLLM
    BuildPrompt3 --> MainLLM

    MainLLM --> End[å›å‚³å›æ‡‰]

    style Gate fill:#ffcc99
    style LLM fill:#99ccff
    style UseMemory fill:#99ff99
    style SkipMemory fill:#ff9999
```

### E.3 æ•ˆèƒ½è©•ä¼° (Performance Evaluation)

**ç†è«–åˆ†æ** (åŸºæ–¼ V1 ç”Ÿç”¢æ•¸æ“š):

| å ´æ™¯ | Memory Gate æ±ºç­– | å¹³å‡å»¶é² | Token æˆæœ¬ | æ­£ç¢ºç‡ |
|------|------------------|----------|-----------|--------|
| æ‰“æ‹›å‘¼ ("æ—©å®‰") | SKIP (è¦å‰‡) | +0ms | 0 | 100% |
| ç—‡ç‹€æè¿° ("ä»Šå¤©å¾ˆå–˜") | USE (è¦å‰‡) | +80ms | +500 tokens | 95% |
| æ¨¡ç³Šå•é¡Œ ("æ€éº¼è¾¦?") | LLM åˆ¤æ–· | +50ms | +10 tokens | 85% |

**æˆæœ¬æ•ˆç›Šæ¯”** (Cost-Benefit Analysis):

å‡è¨­æ¯æ—¥ 1000 æ¬¡å°è©±:
- âŒ **ç„¡ Memory Gate**: 1000 æ¬¡å…¨æª¢ç´¢ â†’ 500k tokens
- âœ… **æœ‰ Memory Gate**: 300 æ¬¡æª¢ç´¢ â†’ 150k tokens + 10k tokens (Gate LLM)
- ğŸ’° **ç¯€çœ**: (500k - 160k) Ã— $0.01/1k = **$3.4/å¤©** (ç´„ **70% æˆæœ¬**)

**çµè«–**: Memory Gate ROI æ¥µé«˜ï¼Œ**å¼·çƒˆå»ºè­°éƒ¨ç½²**ã€‚

---

## F. Audio-level Idempotency

### F.1 ç‚ºä»€éº¼éœ€è¦éŸ³æª”ç´šå†ªç­‰? (Why Audio-level Idempotency?)

**å•é¡Œå ´æ™¯**:
1. ä½¿ç”¨è€…éŒ„è£½ 10 ç§’èªéŸ³ â†’ ä¸Šå‚³æˆåŠŸ
2. LINE Webhook å› ç¶²è·¯ä¸ç©©é‡é€ 3 æ¬¡
3. ç„¡å†ªç­‰ä¿è­· â†’ AI Worker è™•ç† 3 æ¬¡ â†’ ä½¿ç”¨è€…æ”¶åˆ° 3 å‰‡é‡è¤‡å›æ‡‰ âŒ
4. æµªè²» 3Ã— æˆæœ¬ (STT + LLM + TTS)

**è§£æ±ºæ–¹æ¡ˆ**: ä½¿ç”¨ **audio_id** ä½œç‚ºåˆ†æ•£å¼é– (Distributed Lock)ï¼Œä¿è­‰åŒä¸€æ®µèªéŸ³åªè™•ç†ä¸€æ¬¡ã€‚

### F.2 éŸ³æª”ç´šé–å¯¦ä½œ (Audio Lock Implementation)

**V1 å¯¦ä½œ** (ç¹¼æ‰¿è‡³ V2.0):

```python
def acquire_audio_lock(lock_id: str, ttl_sec: int = 180) -> bool:
    """å˜—è©¦ç²å–éŸ³æª”é– (SET NX)ï¼ŒæˆåŠŸå›å‚³ True"""
    r = get_redis()
    key = f"lock:audio:{lock_id}"
    try:
        # SET key "1" NX EX ttl_sec
        # NX: åªåœ¨ä¸å­˜åœ¨æ™‚è¨­å®š (åŸå­æ€§)
        # EX: è¨­å®šéæœŸæ™‚é–“ (é¿å…æ­»é–)
        return bool(r.set(key, "1", nx=True, ex=ttl_sec))
    except Exception:
        return False

def release_audio_lock(lock_id: str) -> None:
    """é‡‹æ”¾éŸ³æª”é– (DEL)"""
    r = get_redis()
    key = f"lock:audio:{lock_id}"
    try:
        r.delete(key)
    except Exception:
        pass  # é‡‹æ”¾å¤±æ•—ä¸å½±éŸ¿æ¥­å‹™ (TTL æœƒè‡ªå‹•æ¸…ç†)
```

**ä½¿ç”¨ç¯„ä¾‹** (åœ¨ AI Worker ä¸­):

```python
def handle_user_message(user_id: str, audio_id: str, query: str) -> str:
    """è™•ç†ä½¿ç”¨è€…èªéŸ³è¨Šæ¯ (ä¿è­‰å†ªç­‰æ€§)"""
    # 1. çµ„åˆé– ID (user_id + audio_id)
    lock_id = f"{user_id}#audio:{audio_id}"

    # 2. å˜—è©¦ç²å–é– (180 ç§’ TTLï¼Œæ¶µè“‹æœ€é•·è™•ç†æ™‚é–“)
    if not acquire_audio_lock(lock_id, ttl_sec=180):
        # é–å·²è¢«å…¶ä»– Worker æŒæœ‰ â†’ æŸ¥è©¢å¿«å–çµæœ
        cached = get_audio_result(user_id, audio_id)
        return cached or "æˆ‘æ­£åœ¨è™•ç†ä½ çš„èªéŸ³ï¼Œè«‹ç¨ç­‰ä¸€ä¸‹å–”ã€‚"

    try:
        # 3. æŒæœ‰é– â†’ åŸ·è¡Œ STT + LLM + TTS
        transcript = stt_service.transcribe(audio_id)
        response = llm_service.generate(user_id, transcript)
        audio_url = tts_service.synthesize(response)

        # 4. å¿«å–çµæœ (24h TTLï¼Œä¾›å¾ŒçºŒæŸ¥è©¢)
        set_audio_result(user_id, audio_id, response, ttl_sec=86400)

        return response

    finally:
        # 5. é‡‹æ”¾é– (å³ä½¿ç•°å¸¸ä¹ŸæœƒåŸ·è¡Œ)
        release_audio_lock(lock_id)
```

### F.3 å†ªç­‰æ€§æ™‚åºåœ– (Idempotency Sequence Diagram)

```mermaid
sequenceDiagram
    participant User
    participant Webhook1 as Webhook Instance 1
    participant Webhook2 as Webhook Instance 2 (é‡é€)
    participant Redis
    participant Worker

    User->>Webhook1: ä¸Šå‚³èªéŸ³ (audio_id=abc123)
    User->>Webhook2: é‡é€ (audio_id=abc123)

    Webhook1->>Redis: SET NX lock:audio:U123#abc123 "1"
    Redis-->>Webhook1: True (ç²å–é–æˆåŠŸ)
    Webhook1->>Worker: è™•ç†ä»»å‹™

    Webhook2->>Redis: SET NX lock:audio:U123#abc123 "1"
    Redis-->>Webhook2: False (é–å·²å­˜åœ¨)
    Webhook2->>Redis: GET audio:U123:abc123:result
    Redis-->>Webhook2: None (å°šæœªå®Œæˆ)
    Webhook2-->>User: "æˆ‘æ­£åœ¨è™•ç†ä½ çš„èªéŸ³ï¼Œè«‹ç¨ç­‰..."

    Worker->>Worker: STT + LLM + TTS (è€—æ™‚ 12s)
    Worker->>Redis: SET audio:U123:abc123:result "é˜¿å…¬æ—©å®‰..."
    Worker->>Redis: DEL lock:audio:U123#abc123

    Webhook1-->>User: "é˜¿å…¬æ—©å®‰ï¼Œä»Šå¤©æœ‰æ¯”è¼ƒå¥½å—?"
```

### F.4 TTL è¨­å®šèˆ‡æ­»é–é é˜² (TTL & Deadlock Prevention)

**ç‚ºä»€éº¼é¸æ“‡ 180 ç§’ TTL?**

| çµ„ä»¶ | æœ€å£æƒ…æ³å»¶é² | ç·©è¡å€æ•¸ | è¨­è¨ˆ TTL |
|------|--------------|----------|----------|
| STT (Whisper API) | 10s | 2Ã— | 20s |
| LLM (GPT-4 Turbo) | 30s | 2Ã— | 60s |
| TTS (OpenAI TTS) | 10s | 2Ã— | 20s |
| RAG æª¢ç´¢ (pgvector) | 5s | 2Ã— | 10s |
| ç¶²è·¯æŠ–å‹• | - | - | 20s |
| **ç¸½è¨ˆ** | **55s** | **~3Ã—** | **180s** |

**é—œéµè¨­è¨ˆ**:
- âœ… TTL > æœ€å£æƒ…æ³ 3 å€ (é¿å…èª¤é‡‹æ”¾)
- âœ… ä½¿ç”¨ `finally` ç¢ºä¿é‡‹æ”¾ (å³ä½¿ç•°å¸¸)
- âœ… é‡‹æ”¾å¤±æ•—ä¸å½±éŸ¿æ¥­å‹™ (TTL å…œåº•)

**æ­»é–å ´æ™¯åˆ†æ**:

| å ´æ™¯ | æ˜¯å¦æ­»é– | åŸå›  | ç·©è§£æªæ–½ |
|------|----------|------|----------|
| Worker å´©æ½°æœªé‡‹æ”¾é– | âŒ ä¸æœƒ | TTL è‡ªå‹•æ¸…ç† | è¨­å®šåˆç† TTL |
| Redis å´©æ½° | âŒ ä¸æœƒ | é‡å•Ÿå¾Œé–ä¸Ÿå¤± | ä½¿ç”¨æŒä¹…åŒ– AOF |
| æƒ¡æ„æŒé–è¶…é TTL | âŒ ä¸æœƒ | TTL å¼·åˆ¶éæœŸ | - |

---

## G. Long-term Memory (PostgreSQL + pgvector)

### G.1 çµæ§‹åŒ–è¨˜æ†¶ (Structured Memory in PostgreSQL)

**è³‡æ–™è¡¨è¨­è¨ˆ** (è©³è¦‹ [database/schema_design_v1.0.md](../database/schema_design_v1.0.md)):

```sql
-- å°è©±æ—¥èªŒè¡¨ (30 å¤©ä¿ç•™)
CREATE TABLE conversation_logs (
    id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(50) NOT NULL,
    request_id VARCHAR(64) NOT NULL UNIQUE,  -- å»é‡ç”¨
    input_text TEXT NOT NULL,
    output_text TEXT NOT NULL,
    is_emergency BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),

    INDEX idx_user_created (user_id, created_at DESC)
);

-- é‡è¦æ™‚åˆ»æ¨™è¨˜ (æ°¸ä¹…ä¿ç•™)
CREATE TABLE important_moments (
    id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(50) NOT NULL,
    conversation_log_id BIGINT REFERENCES conversation_logs(id),
    moment_type VARCHAR(20) NOT NULL,  -- 'symptom_change', 'emergency', 'milestone'
    summary TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

**è³‡æ–™æµ**:

```mermaid
flowchart LR
    Redis[Redis Session] -->|æ¯è¼ªå°è©±| Worker[AI Worker]
    Worker -->|å¯«å…¥| PG[PostgreSQL<br/>conversation_logs]
    Worker -->|é‡è¦æ™‚åˆ»| Important[important_moments]

    PG -->|30å¤©å¾Œ| Archive[æ­¸æª”/åˆªé™¤]
    Important -->|æ°¸ä¹…ä¿ç•™| LongTerm[Long-term Storage]

    style Redis fill:#ff9999
    style PG fill:#99ccff
    style Important fill:#99ff99
```

### G.2 èªç¾©è¨˜æ†¶ (Semantic Memory in pgvector)

**å‘é‡åŒ–ç­–ç•¥**:

```python
from openai import OpenAI

def vectorize_conversation(log: ConversationLog) -> List[float]:
    """å°‡å°è©±è½‰æ›ç‚º 1536 ç¶­å‘é‡ (text-embedding-3-small)"""
    client = OpenAI()

    # çµ„åˆè¼¸å…¥èˆ‡è¼¸å‡ºä½œç‚ºä¸Šä¸‹æ–‡
    text = f"User: {log.input_text}\nAssistant: {log.output_text}"

    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

def store_to_pgvector(log: ConversationLog, vector: List[float]):
    """å„²å­˜å‘é‡åˆ° pgvector"""
    conn.execute("""
        INSERT INTO conversation_embeddings (
            conversation_log_id,
            user_id,
            embedding,
            created_at
        ) VALUES (%s, %s, %s, %s)
    """, (log.id, log.user_id, vector, log.created_at))
```

**èªç¾©æª¢ç´¢** (Semantic Search):

```python
def search_similar_conversations(
    user_id: str,
    query: str,
    top_k: int = 3
) -> List[ConversationLog]:
    """æª¢ç´¢èªç¾©ç›¸ä¼¼çš„æ­·å²å°è©±"""
    # 1. å°‡æŸ¥è©¢å‘é‡åŒ–
    query_vector = vectorize_conversation(query)

    # 2. ä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦æª¢ç´¢ (pgvector <=> operator)
    results = conn.execute("""
        SELECT
            cl.*,
            1 - (ce.embedding <=> %s::vector) AS similarity
        FROM conversation_embeddings ce
        JOIN conversation_logs cl ON ce.conversation_log_id = cl.id
        WHERE ce.user_id = %s
          AND ce.created_at > NOW() - INTERVAL '180 days'  -- åŠå¹´å…§
        ORDER BY ce.embedding <=> %s::vector
        LIMIT %s
    """, (query_vector, user_id, query_vector, top_k))

    return [ConversationLog(**row) for row in results]
```

### G.3 æ··åˆæª¢ç´¢ (Hybrid Retrieval: BM25 + Vector)

**ç‚ºä»€éº¼éœ€è¦æ··åˆæª¢ç´¢?**

| æª¢ç´¢æ–¹å¼ | å„ªé» | ç¼ºé» | é©ç”¨å ´æ™¯ |
|----------|------|------|----------|
| **é—œéµå­— (BM25)** | ç²¾æº–åŒ¹é…ï¼Œå¿«é€Ÿ | ç„¡æ³•ç†è§£èªç¾© | "ä¸Šæ¬¡é‡è¡€æ°§å¤šå°‘?" |
| **å‘é‡ (pgvector)** | èªç¾©ç†è§£ | è¨ˆç®—æˆæœ¬é«˜ | "å‘¼å¸ä¸é †" â‰ˆ "å–˜ä¸éæ°£" |
| **æ··åˆ** | **å…©è€…å„ªé»** | **ç•¥è¤‡é›œ** | **é€šç”¨** |

**V2.0 å¯¦ä½œç­–ç•¥** (åƒè€ƒ [ADR-002](../adr/ADR-002-pgvector-for-vector-db.md)):

```python
def hybrid_search(user_id: str, query: str, top_k: int = 5) -> List[Dict]:
    """æ··åˆæª¢ç´¢: BM25 (40%) + Vector (60%)"""
    # 1. BM25 é—œéµå­—æª¢ç´¢ (PostgreSQL tsvector)
    bm25_results = conn.execute("""
        SELECT id, ts_rank(search_vector, plainto_tsquery('chinese', %s)) AS score
        FROM conversation_logs
        WHERE user_id = %s
          AND search_vector @@ plainto_tsquery('chinese', %s)
        ORDER BY score DESC
        LIMIT %s
    """, (query, user_id, query, top_k * 2))

    # 2. Vector èªç¾©æª¢ç´¢
    vector_results = search_similar_conversations(user_id, query, top_k * 2)

    # 3. èåˆåˆ†æ•¸ (Reciprocal Rank Fusion)
    combined = {}
    for rank, row in enumerate(bm25_results, 1):
        combined[row['id']] = combined.get(row['id'], 0) + 0.4 / (rank + 60)

    for rank, row in enumerate(vector_results, 1):
        combined[row.id] = combined.get(row.id, 0) + 0.6 / (rank + 60)

    # 4. æ’åºä¸¦å›å‚³ Top-K
    sorted_ids = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:top_k]
    return [get_conversation_log(id) for id, _ in sorted_ids]
```

---

## H. TTL & Data Retention Policy

### H.1 è³‡æ–™ç”Ÿå‘½é€±æœŸç®¡ç† (Data Lifecycle Management)

**åˆ†å±¤ä¿ç•™ç­–ç•¥** (Tiered Retention Strategy):

```mermaid
gantt
    title è³‡æ–™ä¿ç•™æ™‚é–“è»¸ (Data Retention Timeline)
    dateFormat  X
    axisFormat %s

    section Redis (Hot)
    Session State (5min)   :0, 300
    Audio Buffer (1h)      :0, 3600
    Audio Result (24h)     :0, 86400
    Conversation (24h)     :0, 86400

    section PostgreSQL (Warm)
    Conversation Logs (30d) :0, 2592000
    Daily Logs (90d)        :0, 7776000
    User Profile (æ°¸ä¹…)     :0, 31536000

    section pgvector (Cold)
    Embeddings (180d)       :0, 15552000
    Important Moments (æ°¸ä¹…) :0, 31536000
```

**ä¿ç•™åŸå‰‡è¡¨** (Retention Policy Table):

| è³‡æ–™é¡å‹ | å„²å­˜å±¤ | TTL | åˆªé™¤ç­–ç•¥ | æ³•è¦ä¾æ“š |
|----------|--------|-----|----------|----------|
| Session State | Redis | 5 min | è‡ªå‹•éæœŸ | - |
| å°è©±æ­·å² (åŸå§‹) | Redis | 24 h | è‡ªå‹•éæœŸ | - |
| å°è©±æ—¥èªŒ (çµæ§‹åŒ–) | PostgreSQL | 30 å¤© | å®šæœŸæ¸…ç† Job | å°ç£å€‹è³‡æ³• (æœ€å°ä¿ç•™åŸå‰‡) |
| èªç¾©å‘é‡ | pgvector | 180 å¤© | å®šæœŸæ¸…ç† Job | - |
| é‡è¦æ™‚åˆ» | PostgreSQL | æ°¸ä¹… | ä½¿ç”¨è€…ä¸»å‹•åˆªé™¤ | é†«ç™‚ç´€éŒ„ä¿ç•™ç¾©å‹™ |
| å¥åº·æ—¥èªŒ | PostgreSQL | 90 å¤© | å®šæœŸæ¸…ç† Job | - |
| å•å·å›æ‡‰ | PostgreSQL | æ°¸ä¹… | - | è‡¨åºŠç ”ç©¶éœ€æ±‚ |

**å®šæœŸæ¸…ç† Job** (Cron Job):

```python
# scripts/cleanup_old_data.py
from datetime import datetime, timedelta

def cleanup_old_conversations():
    """æ¯æ—¥æ¸…ç† 30 å¤©å‰çš„å°è©±æ—¥èªŒ"""
    cutoff_date = datetime.now() - timedelta(days=30)

    conn.execute("""
        DELETE FROM conversation_logs
        WHERE created_at < %s
          AND id NOT IN (
              SELECT conversation_log_id
              FROM important_moments
              WHERE conversation_log_id IS NOT NULL
          )
    """, (cutoff_date,))

    print(f"Deleted conversations older than {cutoff_date}")

def cleanup_old_embeddings():
    """æ¯é€±æ¸…ç† 180 å¤©å‰çš„èªç¾©å‘é‡"""
    cutoff_date = datetime.now() - timedelta(days=180)

    conn.execute("""
        DELETE FROM conversation_embeddings
        WHERE created_at < %s
    """, (cutoff_date,))

    print(f"Deleted embeddings older than {cutoff_date}")

# Crontab è¨­å®š
# 0 2 * * * /usr/bin/python /app/scripts/cleanup_old_data.py
```

### H.2 ä½¿ç”¨è€…è³‡æ–™ä¸»å‹•æ¸…é™¤ (User-initiated Data Deletion)

**GDPR/å°ç£å€‹è³‡æ³•åˆè¦** (Data Subject Rights):

```python
def delete_user_data(user_id: str, scope: str = "all"):
    """ä½¿ç”¨è€…è¡Œä½¿ã€Œè¢«éºå¿˜æ¬Šã€(Right to be Forgotten)"""
    if scope == "all":
        # 1. æ¸…é™¤ Redis æ‰€æœ‰ Session è³‡æ–™
        purge_user_session(user_id)

        # 2. æ¸…é™¤ PostgreSQL å°è©±æ—¥èªŒ (ä¿ç•™é‡è¦æ™‚åˆ»æ¨™è¨˜)
        conn.execute("""
            DELETE FROM conversation_logs
            WHERE user_id = %s
              AND id NOT IN (
                  SELECT conversation_log_id
                  FROM important_moments
              )
        """, (user_id,))

        # 3. æ¸…é™¤ pgvector èªç¾©å‘é‡
        conn.execute("""
            DELETE FROM conversation_embeddings
            WHERE user_id = %s
        """, (user_id,))

        # 4. åŒ¿ååŒ–é‡è¦æ™‚åˆ» (ä¿ç•™çµ±è¨ˆç”¨é€”)
        conn.execute("""
            UPDATE important_moments
            SET user_id = 'ANONYMIZED'
            WHERE user_id = %s
        """, (user_id,))

        print(f"User {user_id} data deleted (important moments anonymized)")

    elif scope == "recent":
        # åƒ…æ¸…é™¤æœ€è¿‘ 7 å¤©å°è©±
        cutoff = datetime.now() - timedelta(days=7)
        conn.execute("""
            DELETE FROM conversation_logs
            WHERE user_id = %s AND created_at > %s
        """, (user_id, cutoff))
```

---

## I. Performance & Scalability

### I.1 æ•ˆèƒ½åŸºæº– (Performance Baselines)

**ç›®æ¨™ SLI (Service Level Indicators)**:

| æ“ä½œ | P50 å»¶é² | P95 å»¶é² | P99 å»¶é² | å‚™è¨» |
|------|----------|----------|----------|------|
| Redis Session è®€å– | < 5ms | < 10ms | < 20ms | LRANGE æœ€è¿‘ 6 è¼ª |
| Redis Session å¯«å…¥ | < 10ms | < 20ms | < 50ms | RPUSH + æ‰¹æ¬¡ TTL |
| PostgreSQL å¯«å…¥æ—¥èªŒ | < 50ms | < 100ms | < 200ms | å–®ç­† INSERT |
| pgvector èªç¾©æª¢ç´¢ | < 200ms | < 500ms | < 1s | Top-5 ç›¸ä¼¼å°è©± |
| å®Œæ•´ AI å›æ‡‰ (å«è¨˜æ†¶) | < 10s | < 15s | < 20s | STT + LLM + TTS |

**è² è¼‰æ¸¬è©¦è¨ˆç•«** (Load Testing Plan):

```python
# tests/load_test_memory.py
from locust import HttpUser, task, between

class MemoryUser(HttpUser):
    wait_time = between(5, 10)  # ä½¿ç”¨è€…é–“éš” 5-10s ç™¼é€è¨Šæ¯

    @task(3)
    def send_normal_message(self):
        """ä¸€èˆ¬å°è©± (70% æµé‡)"""
        self.client.post("/api/voice", json={
            "user_id": f"U{self.user_id}",
            "audio_id": generate_audio_id(),
            "text": "ä»Šå¤©å‘¼å¸æœ‰æ¯”è¼ƒé †",
            "is_final": True
        })

    @task(1)
    def send_memory_query(self):
        """è¨˜æ†¶æŸ¥è©¢ (30% æµé‡)"""
        self.client.post("/api/voice", json={
            "user_id": f"U{self.user_id}",
            "audio_id": generate_audio_id(),
            "text": "ä¸Šæ¬¡ä½ èªªæˆ‘è¡€æ°§åä½ï¼Œç¾åœ¨æ€éº¼è¾¦?",
            "is_final": True
        })

# åŸ·è¡Œè² è¼‰æ¸¬è©¦
# locust -f tests/load_test_memory.py --users 500 --spawn-rate 10
```

### I.2 æ“´å±•ç­–ç•¥ (Scalability Strategy)

**æ°´å¹³æ“´å±• (Horizontal Scaling)**:

```mermaid
graph TB
    subgraph "Load Balancer"
        LB[Nginx]
    end

    subgraph "API Servers (Stateless)"
        API1[FastAPI 1]
        API2[FastAPI 2]
        API3[FastAPI 3]
    end

    subgraph "AI Workers (Stateless)"
        Worker1[Worker 1]
        Worker2[Worker 2]
        Worker3[Worker 3]
    end

    subgraph "Shared State"
        Redis[(Redis Cluster<br/>3 Master + 3 Replica)]
        PG[(PostgreSQL<br/>Primary + 2 Replicas)]
        pgvector[(pgvector<br/>Read Replicas)]
    end

    LB --> API1 & API2 & API3
    API1 & API2 & API3 --> Redis
    API1 & API2 & API3 --> Worker1 & Worker2 & Worker3
    Worker1 & Worker2 & Worker3 --> Redis
    Worker1 & Worker2 & Worker3 --> PG
    Worker1 & Worker2 & Worker3 --> pgvector

    style Redis fill:#ff9999
    style PG fill:#99ccff
    style pgvector fill:#99ff99
```

**å®¹é‡è¦åŠƒ** (Capacity Planning):

| çµ„ä»¶ | 500 CCU éœ€æ±‚ | å–®å¯¦ä¾‹å®¹é‡ | å¯¦ä¾‹æ•¸é‡ | æˆæœ¬ä¼°ç®— |
|------|--------------|------------|----------|----------|
| FastAPI Server | 500 req/s | 200 req/s | 3Ã— | $30/æœˆ Ã— 3 |
| AI Worker | 50 voice/s | 20 voice/s | 3Ã— | $50/æœˆ Ã— 3 |
| Redis Cluster | 10k ops/s | 50k ops/s | 1Ã— (6 nodes) | $80/æœˆ |
| PostgreSQL | 500 writes/s | 2k writes/s | 1Ã— (primary + 2 replica) | $150/æœˆ |
| pgvector | 100 queries/s | 200 queries/s | 1Ã— (read replica) | $100/æœˆ |
| **ç¸½è¨ˆ** | - | - | - | **$590/æœˆ** |

**ç“¶é ¸åˆ†æ** (Bottleneck Analysis):

1. **LLM API é€Ÿç‡é™åˆ¶**: OpenAI GPT-4 Turbo é™åˆ¶ 500 RPM (æ¯åˆ†é˜è«‹æ±‚æ•¸)
   - ç·©è§£: ä½¿ç”¨å¤š API Key è¼ªè©¢ (Round-robin)
   - ç·©è§£: é«˜å³°æ™‚æ®µé™ç´šç‚º GPT-4o-mini

2. **pgvector æŸ¥è©¢å»¶é²**: å‘é‡æª¢ç´¢éš¨è³‡æ–™é‡å¢é•·è®Šæ…¢
   - ç·©è§£: ä½¿ç”¨ HNSW ç´¢å¼• (Hierarchical Navigable Small World)
   - ç·©è§£: åˆ†å€å„²å­˜ (æŒ‰ä½¿ç”¨è€… ID hash åˆ†ç‰‡)

3. **Redis è¨˜æ†¶é«”é™åˆ¶**: 500 CCU Ã— 24h å°è©± â‰ˆ 5GB
   - ç·©è§£: ä½¿ç”¨ Redis Cluster åˆ†ç‰‡
   - ç·©è§£: æ¿€é€²çš„ TTL ç­–ç•¥ (12h â†’ 6h)

---

## J. Security & Privacy Considerations

### J.1 è¨˜æ†¶é«”è³‡æ–™åŠ å¯† (Memory Data Encryption)

**åŠ å¯†å±¤ç´š** (Encryption Layers):

| å±¤ç´š | åŠ å¯†æ–¹å¼ | é‡‘é‘°ç®¡ç† | é©ç”¨è³‡æ–™ |
|------|----------|----------|----------|
| **å‚³è¼¸å±¤** | TLS 1.3 | Let's Encrypt | API â†” Redis, API â†” PostgreSQL |
| **éœæ…‹å„²å­˜** | AES-256 | AWS KMS / Vault | Redis RDB, PostgreSQL Data |
| **æ‡‰ç”¨å±¤** | æ¬„ä½ç´šåŠ å¯† | ç’°å¢ƒè®Šæ•¸ | æ•æ„Ÿå°è©±å…§å®¹ (PII) |

**æ¬„ä½ç´šåŠ å¯†å¯¦ä½œ** (Field-level Encryption):

```python
from cryptography.fernet import Fernet
import os

class EncryptedMemory:
    def __init__(self):
        # å¾ç’°å¢ƒè®Šæ•¸è®€å–åŠ å¯†é‡‘é‘° (32 bytes base64)
        key = os.getenv("MEMORY_ENCRYPTION_KEY").encode()
        self.cipher = Fernet(key)

    def encrypt_conversation(self, text: str) -> str:
        """åŠ å¯†å°è©±å…§å®¹"""
        return self.cipher.encrypt(text.encode()).decode()

    def decrypt_conversation(self, encrypted: str) -> str:
        """è§£å¯†å°è©±å…§å®¹"""
        return self.cipher.decrypt(encrypted.encode()).decode()

# ä½¿ç”¨ç¯„ä¾‹
def append_round_encrypted(user_id: str, input_text: str, output_text: str):
    """åŠ å¯†å¾Œå„²å­˜å°è©±"""
    enc = EncryptedMemory()
    round_obj = {
        "input": enc.encrypt_conversation(input_text),
        "output": enc.encrypt_conversation(output_text),
        "rid": make_request_id(user_id, input_text),
        "timestamp": int(time.time() * 1000)
    }
    append_round(user_id, round_obj)
```

### J.2 å­˜å–æ§åˆ¶ (Access Control)

**æœ€å°æ¬Šé™åŸå‰‡** (Principle of Least Privilege):

| è§’è‰² | Redis æ¬Šé™ | PostgreSQL æ¬Šé™ | pgvector æ¬Šé™ |
|------|-----------|----------------|---------------|
| **AI Worker** | è®€å¯« session:*, audio:* | INSERT conversation_logs | SELECT, INSERT embeddings |
| **API Server** | è®€å– session:* | SELECT æ‰€æœ‰è¡¨ | SELECT æ‰€æœ‰è¡¨ |
| **æ²»ç™‚å¸« Dashboard** | - | SELECT patient_*, daily_logs* | - |
| **è³‡æ–™ç§‘å­¸å®¶** | - | SELECT (åŒ¿ååŒ–) | SELECT (åŒ¿ååŒ–) |

**Redis ACL è¨­å®š**:

```bash
# redis.conf
user ai_worker on >STRONG_PASSWORD ~session:* ~audio:* +@all
user api_server on >STRONG_PASSWORD ~session:* +@read
user default off  # ç¦ç”¨é è¨­ä½¿ç”¨è€…
```

**PostgreSQL Row-level Security**:

```sql
-- æ²»ç™‚å¸«åªèƒ½æŸ¥çœ‹è‡ªå·±è² è²¬çš„ç—…æ‚£
ALTER TABLE conversation_logs ENABLE ROW LEVEL SECURITY;

CREATE POLICY therapist_access ON conversation_logs
    FOR SELECT
    TO therapist_role
    USING (
        user_id IN (
            SELECT patient_id
            FROM patient_assignments
            WHERE therapist_id = current_user
        )
    );
```

### J.3 ç¨½æ ¸æ—¥èªŒ (Audit Logging)

**è¨˜æ†¶é«”å­˜å–ç¨½æ ¸** (Memory Access Audit):

```python
def audit_log(action: str, user_id: str, data_type: str, success: bool):
    """è¨˜éŒ„æ•æ„Ÿæ“ä½œåˆ° MongoDB (ä¸å¯è®Šæ—¥èªŒ)"""
    mongo_client.audit_logs.insert_one({
        "action": action,  # "READ_MEMORY", "DELETE_SESSION", "EXPORT_DATA"
        "user_id": user_id,
        "data_type": data_type,
        "success": success,
        "timestamp": datetime.utcnow(),
        "ip_address": request.remote_addr,
        "user_agent": request.headers.get("User-Agent")
    })

# ä½¿ç”¨ç¯„ä¾‹
def fetch_user_conversations(user_id: str, therapist_id: str):
    """æ²»ç™‚å¸«æŸ¥çœ‹ç—…æ‚£å°è©± (è¨˜éŒ„ç¨½æ ¸æ—¥èªŒ)"""
    try:
        logs = get_conversation_logs(user_id)
        audit_log("READ_MEMORY", user_id, "conversation_logs", True)
        return logs
    except PermissionError:
        audit_log("READ_MEMORY", user_id, "conversation_logs", False)
        raise
```

---

## K. Monitoring & Observability

### K.1 é—œéµæŒ‡æ¨™ (Key Metrics)

**è¨˜æ†¶é«”å¥åº·åº¦æŒ‡æ¨™** (Memory Health Metrics):

```python
from prometheus_client import Counter, Histogram, Gauge

# 1. å°è©±é•·åº¦åˆ†å¸ƒ (Conversation Length Distribution)
conversation_length = Histogram(
    'memory_conversation_length_rounds',
    'Number of rounds in a conversation',
    buckets=[1, 5, 10, 20, 50, 100]
)

# 2. æ‘˜è¦è§¸ç™¼é »ç‡ (Summary Trigger Rate)
summary_triggered = Counter(
    'memory_summary_triggered_total',
    'Number of times rolling summary was triggered'
)

# 3. Memory Gate æ±ºç­–åˆ†å¸ƒ (Memory Gate Decisions)
memory_gate_decision = Counter(
    'memory_gate_decision_total',
    'Memory Gate decisions',
    ['decision']  # 'USE' or 'SKIP'
)

# 4. Redis è¨˜æ†¶é«”ä½¿ç”¨é‡ (Redis Memory Usage)
redis_memory_bytes = Gauge(
    'redis_memory_used_bytes',
    'Redis memory usage in bytes'
)

# 5. pgvector æŸ¥è©¢å»¶é² (pgvector Query Latency)
pgvector_query_duration = Histogram(
    'pgvector_query_duration_seconds',
    'pgvector semantic search latency',
    buckets=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
)
```

### K.2 å‘Šè­¦è¦å‰‡ (Alerting Rules)

**Prometheus Alerting Rules**:

```yaml
# prometheus/alerts/memory_alerts.yml
groups:
  - name: memory_health
    interval: 30s
    rules:
      # Alert 1: Redis è¨˜æ†¶é«”ä½¿ç”¨ç‡ > 80%
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage above 80%"
          description: "Current usage: {{ $value | humanizePercentage }}"

      # Alert 2: pgvector æŸ¥è©¢ P95 å»¶é² > 1s
      - alert: PgvectorSlowQuery
        expr: histogram_quantile(0.95, pgvector_query_duration_seconds) > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "pgvector P95 latency > 1s"

      # Alert 3: å°è©±é•·åº¦ç•°å¸¸ (> 100 è¼ªæœªæ‘˜è¦)
      - alert: ConversationTooLong
        expr: max(memory_conversation_length_rounds) > 100
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Conversation exceeds 100 rounds without summary"
          description: "Check rolling summary mechanism"

      # Alert 4: Memory Gate å…¨éƒ¨ SKIP (å¯èƒ½æ•…éšœ)
      - alert: MemoryGateAllSkip
        expr: |
          rate(memory_gate_decision_total{decision="SKIP"}[5m]) /
          rate(memory_gate_decision_total[5m]) > 0.95
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Memory Gate skipping 95% of queries"
```

### K.3 å¯è¦–åŒ–å„€è¡¨æ¿ (Visualization Dashboards)

**Grafana Dashboard JSON** (ç¯„ä¾‹é¢æ¿):

```json
{
  "dashboard": {
    "title": "Memory Management Dashboard",
    "panels": [
      {
        "title": "Conversation Length Distribution",
        "type": "histogram",
        "targets": [{
          "expr": "memory_conversation_length_rounds"
        }]
      },
      {
        "title": "Memory Gate Decisions (Pie Chart)",
        "type": "piechart",
        "targets": [{
          "expr": "sum by (decision) (memory_gate_decision_total)"
        }]
      },
      {
        "title": "Redis Memory Usage Over Time",
        "type": "graph",
        "targets": [{
          "expr": "redis_memory_used_bytes / 1024 / 1024",
          "legendFormat": "Memory (MB)"
        }]
      },
      {
        "title": "pgvector Query Latency (P50/P95/P99)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, pgvector_query_duration_seconds)",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.95, pgvector_query_duration_seconds)",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, pgvector_query_duration_seconds)",
            "legendFormat": "P99"
          }
        ]
      }
    ]
  }
}
```

---

## L. Migration from V1

### L.1 V1 vs V2 æ¶æ§‹å°æ¯” (Architecture Comparison)

| ç¶­åº¦ | V1 (beloved_grandson) | V2 (RespiraAlly) | è®Šæ›´åŸå›  |
|------|----------------------|------------------|----------|
| **è¨˜æ†¶é«”å„²å­˜** | Redis å–®å¯¦ä¾‹ | Redis Cluster (3M + 3R) | é«˜å¯ç”¨æ€§ |
| **å°è©±æ—¥èªŒ** | åƒ… Redis (24h) | Redis (24h) + PostgreSQL (30d) | é•·æœŸåˆ†æéœ€æ±‚ |
| **å‘é‡æœå°‹** | Milvus | pgvector (PostgreSQL) | ç°¡åŒ–æ¶æ§‹ï¼Œæ¸›å°‘çµ„ä»¶ |
| **å»é‡çª—å£** | 3 ç§’ | 3 ç§’ (ä¿æŒä¸è®Š) | âœ… å·²é©—è­‰æœ‰æ•ˆ |
| **æ‘˜è¦ Chunk** | 5 è¼ª | 5 è¼ª (ä¿æŒä¸è®Š) | âœ… å·²é©—è­‰æœ‰æ•ˆ |
| **éŸ³æª”é– TTL** | 60 ç§’ | 180 ç§’ | æ¶µè“‹ RAG æª¢ç´¢å»¶é² |
| **åŠ å¯†** | ç„¡ | æ¬„ä½ç´š AES-256 | åˆè¦éœ€æ±‚ |

### L.2 è³‡æ–™é·ç§»è¨ˆç•« (Data Migration Plan)

**Phase 1: Schema å»ºç«‹** (Week 1)

```sql
-- 1. å»ºç«‹ PostgreSQL å°è©±æ—¥èªŒè¡¨
CREATE TABLE conversation_logs (
    id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(50) NOT NULL,
    request_id VARCHAR(64) NOT NULL UNIQUE,
    input_text TEXT NOT NULL,
    output_text TEXT NOT NULL,
    is_emergency BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- 2. å•Ÿç”¨ pgvector æ“´å……
CREATE EXTENSION IF NOT EXISTS vector;

-- 3. å»ºç«‹å‘é‡è¡¨
CREATE TABLE conversation_embeddings (
    id BIGSERIAL PRIMARY KEY,
    conversation_log_id BIGINT REFERENCES conversation_logs(id),
    user_id VARCHAR(50) NOT NULL,
    embedding vector(1536),  -- OpenAI text-embedding-3-small
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- 4. å»ºç«‹ç´¢å¼•
CREATE INDEX idx_embeddings_user_id ON conversation_embeddings(user_id);
CREATE INDEX idx_embeddings_vector ON conversation_embeddings
    USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

**Phase 2: é›™å¯«æ¨¡å¼** (Week 2-4)

```python
def append_round_dual_write(user_id: str, round_obj: Dict):
    """é›™å¯«: åŒæ™‚å¯«å…¥ Redis (V1) å’Œ PostgreSQL (V2)"""
    # 1. å¯«å…¥ Redis (ç¾æœ‰é‚è¼¯)
    append_round(user_id, round_obj)

    # 2. å¯«å…¥ PostgreSQL (æ–°é‚è¼¯)
    try:
        conn.execute("""
            INSERT INTO conversation_logs (
                user_id, request_id, input_text, output_text, created_at
            ) VALUES (%s, %s, %s, %s, %s)
        """, (
            user_id,
            round_obj['rid'],
            round_obj['input'],
            round_obj['output'],
            datetime.fromtimestamp(round_obj['timestamp'] / 1000)
        ))
    except Exception as e:
        # è¨˜éŒ„éŒ¯èª¤ä½†ä¸å½±éŸ¿ä¸»æµç¨‹
        logger.error(f"PostgreSQL write failed: {e}")
```

**Phase 3: è³‡æ–™é©—è­‰** (Week 5)

```python
def validate_migration():
    """é©—è­‰ Redis èˆ‡ PostgreSQL è³‡æ–™ä¸€è‡´æ€§"""
    # æŠ½æ¨£ 100 ä½ä½¿ç”¨è€…
    sample_users = random.sample(all_users, 100)

    for user_id in sample_users:
        # 1. å¾ Redis è®€å–æœ€è¿‘ 24h å°è©±
        redis_logs = fetch_all_history(user_id)

        # 2. å¾ PostgreSQL è®€å–æœ€è¿‘ 24h å°è©±
        pg_logs = conn.execute("""
            SELECT * FROM conversation_logs
            WHERE user_id = %s
              AND created_at > NOW() - INTERVAL '24 hours'
            ORDER BY created_at ASC
        """, (user_id,)).fetchall()

        # 3. æ¯”å°æ•¸é‡èˆ‡å…§å®¹
        assert len(redis_logs) == len(pg_logs), f"Count mismatch for {user_id}"
        for r, p in zip(redis_logs, pg_logs):
            assert r['rid'] == p['request_id'], f"RID mismatch for {user_id}"

    print("âœ… Migration validation passed")
```

**Phase 4: åˆ‡æ›æµé‡** (Week 6)

```python
# Feature flag æ§åˆ¶è®€å–ä¾†æº
USE_POSTGRESQL_MEMORY = os.getenv("USE_POSTGRESQL_MEMORY", "false") == "true"

def fetch_conversation_history(user_id: str, k: int = 6):
    """æ ¹æ“š Feature flag æ±ºå®šå¾ Redis æˆ– PostgreSQL è®€å–"""
    if USE_POSTGRESQL_MEMORY:
        # V2: å¾ PostgreSQL è®€å–
        return fetch_from_postgresql(user_id, k)
    else:
        # V1: å¾ Redis è®€å–
        return fetch_unsummarized_tail(user_id, k)
```

---

## M. Review Conclusion & Action Items

### M.1 è¨­è¨ˆå¯©æŸ¥çµè«– (Design Review Conclusion)

**å„ªå‹¢** (Strengths):

âœ… **æˆç†Ÿåº¦é«˜**: ç¹¼æ‰¿ V1 å·²é©—è­‰çš„è¨­è¨ˆæ¨¡å¼ (3s å»é‡ã€5 è¼ªæ‘˜è¦ã€éŸ³æª”é–)
âœ… **å¯æ“´å±•æ€§**: ä¸‰å±¤æ¶æ§‹ (Redis/PostgreSQL/pgvector) æ”¯æ´å¾ 100 CCU åˆ° 10k CCU
âœ… **æˆæœ¬å„ªåŒ–**: Memory Gate ç¯€çœ 70% Token æˆæœ¬
âœ… **åˆè¦æ€§**: æ”¯æ´ GDPR/å°ç£å€‹è³‡æ³• (è¢«éºå¿˜æ¬Šã€è³‡æ–™åŒ¯å‡º)
âœ… **å¯è§€æ¸¬æ€§**: å®Œæ•´çš„ Prometheus æŒ‡æ¨™èˆ‡ Grafana å„€è¡¨æ¿

**é¢¨éšªèˆ‡ç·©è§£** (Risks & Mitigations):

| é¢¨éšª | å½±éŸ¿ | ç·©è§£æªæ–½ | å„ªå…ˆç´š |
|------|------|----------|--------|
| **Redis å–®é»æ•…éšœ** | é«˜ (æœå‹™ä¸­æ–·) | éƒ¨ç½² Redis Cluster (3M + 3R) | P0 |
| **LLM API é€Ÿç‡é™åˆ¶** | ä¸­ (é«˜å³°é™ç´š) | å¤š API Key è¼ªè©¢ | P1 |
| **pgvector æŸ¥è©¢è®Šæ…¢** | ä¸­ (å»¶é²å¢åŠ ) | HNSW ç´¢å¼• + åˆ†ç‰‡ | P1 |
| **åŠ å¯†é‡‘é‘°æ´©éœ²** | é«˜ (è³‡æ–™å¤–æ´©) | ä½¿ç”¨ AWS KMS ç®¡ç†é‡‘é‘° | P0 |
| **è³‡æ–™é·ç§»å¤±æ•—** | ä¸­ (å›æ»¾) | é›™å¯«æ¨¡å¼ + Feature flag | P1 |

### M.2 Action Items (è¡Œå‹•é …ç›®)

**Phase 1: MVP (Sprint 1-2, Week 1-4)**

| ä»»å‹™ | è² è²¬äºº | å·¥æ™‚ | å„ªå…ˆç´š | é©—æ”¶æ¨™æº– |
|------|--------|------|--------|----------|
| å¯¦ä½œ Redis Key è¨­è¨ˆè¦ç¯„ | Backend | 8h | P0 | é€šéå–®å…ƒæ¸¬è©¦ |
| å¯¦ä½œ 3s å»é‡æ©Ÿåˆ¶ | Backend | 4h | P0 | é€šéæ•´åˆæ¸¬è©¦ |
| å¯¦ä½œæ»¾å‹•æ‘˜è¦ (5 è¼ª Chunk) | Backend | 8h | P0 | å£“ç¸®æ¯” > 80% |
| å¯¦ä½œéŸ³æª”ç´šé– (180s TTL) | Backend | 4h | P0 | ä¸¦ç™¼æ¸¬è©¦é€šé |
| å¯¦ä½œ Memory Gate (è¦å‰‡ + LLM) | AI/ML | 12h | P1 | æº–ç¢ºç‡ > 85% |
| PostgreSQL Schema å»ºç«‹ | Backend | 4h | P0 | Schema é©—è­‰é€šé |
| pgvector æ•´åˆ | Backend | 8h | P1 | èªç¾©æª¢ç´¢æ¸¬è©¦é€šé |
| Prometheus æŒ‡æ¨™åŸ‹é» | DevOps | 6h | P1 | Grafana å„€è¡¨æ¿å®Œæˆ |

**Phase 2: Production Hardening (Sprint 3-4, Week 5-8)**

| ä»»å‹™ | è² è²¬äºº | å·¥æ™‚ | å„ªå…ˆç´š | é©—æ”¶æ¨™æº– |
|------|--------|------|--------|----------|
| Redis Cluster éƒ¨ç½² | DevOps | 12h | P0 | æ•…éšœåˆ‡æ›æ¸¬è©¦é€šé |
| æ¬„ä½ç´šåŠ å¯†å¯¦ä½œ | Backend | 8h | P0 | å®‰å…¨ç¨½æ ¸é€šé |
| Row-level Security è¨­å®š | Backend | 6h | P1 | æ¬Šé™æ¸¬è©¦é€šé |
| ç¨½æ ¸æ—¥èªŒå¯¦ä½œ | Backend | 6h | P1 | MongoDB å¯«å…¥æˆåŠŸ |
| è² è¼‰æ¸¬è©¦ (500 CCU) | QA | 16h | P0 | æ‰€æœ‰ SLI é”æ¨™ |
| è³‡æ–™é·ç§»è…³æœ¬ | Backend | 8h | P1 | é©—è­‰æ¸¬è©¦é€šé |

**Phase 3: Optimization (Sprint 5-6, Week 9-12)**

| ä»»å‹™ | è² è²¬äºº | å·¥æ™‚ | å„ªå…ˆç´š | é©—æ”¶æ¨™æº– |
|------|--------|------|--------|----------|
| Memory Gate æ¨¡å‹èª¿å„ª | AI/ML | 12h | P2 | æº–ç¢ºç‡ > 90% |
| pgvector HNSW ç´¢å¼•å„ªåŒ– | Backend | 8h | P2 | P95 å»¶é² < 500ms |
| æ··åˆæª¢ç´¢ (BM25 + Vector) | Backend | 12h | P2 | å¬å›ç‡ > 85% |
| Grafana å‘Šè­¦è¦å‰‡ | DevOps | 4h | P2 | å‘Šè­¦æ¸¬è©¦é€šé |
| å®šæœŸæ¸…ç† Job | Backend | 4h | P2 | Cron åŸ·è¡ŒæˆåŠŸ |

### M.3 å¾ŒçºŒç ”ç©¶æ–¹å‘ (Future Research)

1. **å¤šæ¨¡æ…‹è¨˜æ†¶** (Multimodal Memory): æ•´åˆåœ–ç‰‡ã€èªéŸ³ç‰¹å¾µåˆ°èªç¾©å‘é‡
2. **è¯é‚¦å­¸ç¿’** (Federated Learning): åœ¨ä¸æ´©éœ²éš±ç§å‰æä¸‹è¨“ç·´å€‹äººåŒ–æ¨¡å‹
3. **è‡ªé©æ‡‰æ‘˜è¦** (Adaptive Summarization): æ ¹æ“šå°è©±é‡è¦æ€§å‹•æ…‹èª¿æ•´ Chunk å¤§å°
4. **åœ–è¨˜æ†¶é«”** (Graph Memory): ä½¿ç”¨çŸ¥è­˜åœ–è­œå„²å­˜å¯¦é«”é—œä¿‚ (å¦‚ "é˜¿å…¬ - æ‚£æœ‰ - COPD")

---

## é™„éŒ„ A: åƒè€ƒå¯¦ä½œ (Reference Implementations)

### A.1 V1 Redis Store å®Œæ•´ç¨‹å¼ç¢¼

åƒè€ƒæª”æ¡ˆ: `/mnt/a/AIPE01_æœŸæœ«å°ˆé¡Œ/beloved_grandson/services/ai-worker/worker/llm_app/toolkits/redis_store.py`

### A.2 V1 Chat Pipeline å®Œæ•´ç¨‹å¼ç¢¼

åƒè€ƒæª”æ¡ˆ: `/mnt/a/AIPE01_æœŸæœ«å°ˆé¡Œ/beloved_grandson/services/ai-worker/worker/llm_app/chat_pipeline.py`

---

## é™„éŒ„ B: è¡“èªè¡¨ (Glossary)

| è¡“èª | è‹±æ–‡ | å®šç¾© |
|------|------|------|
| å»é‡ | Deduplication | é¿å…é‡è¤‡è™•ç†ç›¸åŒè«‹æ±‚çš„æ©Ÿåˆ¶ |
| å†ªç­‰æ€§ | Idempotency | å¤šæ¬¡åŸ·è¡Œç›¸åŒæ“ä½œçµæœä¸€è‡´çš„æ€§è³ª |
| æ»¾å‹•æ‘˜è¦ | Rolling Summary | å®šæœŸå£“ç¸®èˆŠå°è©±ç‚ºæ‘˜è¦çš„ç­–ç•¥ |
| èªç¾©æª¢ç´¢ | Semantic Search | åŸºæ–¼å‘é‡ç›¸ä¼¼åº¦çš„æœå°‹æ–¹å¼ |
| æ¨‚è§€é– | Optimistic Locking | ä½¿ç”¨ CAS æ©Ÿåˆ¶é¿å…ä¸¦ç™¼è¡çª |
| æ™‚é–“çª—å£ | Time Bucket | å°‡æ™‚é–“è»¸åˆ‡åˆ†ç‚ºå›ºå®šå€é–“ |
| TTL | Time-To-Live | è³‡æ–™è‡ªå‹•éæœŸæ™‚é–“ |
| CAS | Compare-And-Swap | åŸå­æ€§æ¢ä»¶æ›´æ–°æ“ä½œ |
| HNSW | Hierarchical Navigable Small World | é«˜æ•ˆå‘é‡ç´¢å¼•æ¼”ç®—æ³• |
| RRF | Reciprocal Rank Fusion | æ··åˆæª¢ç´¢çš„åˆ†æ•¸èåˆæ–¹æ³• |

---

**Document End**
**Total Words:** ~8,500
**Total Sections:** 13 (A-M)
**Review Status:** âœ… Ready for Technical Review
**Next Steps:** æäº¤çµ¦ System Architect èˆ‡ AI/ML Engineer å¯©æŸ¥
